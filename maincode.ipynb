{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "import math\n",
    "import sympy \n",
    "from sympy import ifft\n",
    "from sympy import fft\n",
    "from numpy.fft import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from joblib import parallel_backend\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "early_stopping = EarlyStopping()\n",
    "scaler = MinMaxScaler()\n",
    "random.seed(42)\n",
    "\n",
    "# Load datasets\n",
    "padsiztrain = pd.read_csv('dayss.csv')  # train with pad\n",
    "padlitrain = pd.read_csv('dayss.csv')  # train without pad\n",
    "test = pd.read_csv('nextweekday2.csv')  # test with pad\n",
    "\n",
    "# Set parameters\n",
    "strategy= 0 #1 for Padding, 0 for Differential Privacy (FPA)\n",
    "pad= 300 #100, 300, 350, 500, 700, 900, 42, 1500\n",
    "vector=20 #Vector Size: 5, 10, 15, 20, 25, 30, 40, 50\n",
    "k=5 #coefficients: 10, 20\n",
    "epsilon=10 #epsilon: 0.05, 0.5, 5, 10\"\"\"\n",
    "clip_top = 1500  \n",
    "clip_bottom = 0\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(data, clip_bottom, clip_top, drop_devices):\n",
    "    data['Size'] = np.clip(data['Size'], clip_bottom, clip_top)\n",
    "    data = data.loc[~data['device'].isin(drop_devices)]\n",
    "    data['device'] = data['device'].replace([4,5,6,7,8,9,11,12,13,14,16,17,18,20],[3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "    data = data.sort_values(by=['device','TIME']).reset_index()\n",
    "    return data\n",
    "\n",
    "# Preprocess the data\n",
    "drop_devices = [3,10,15,19]\n",
    "padsiztrain = preprocess_data(padsiztrain, clip_bottom, clip_top, drop_devices)\n",
    "padlitrain = preprocess_data(padlitrain, clip_bottom, clip_top, drop_devices)\n",
    "test = preprocess_data(test, clip_bottom, clip_top, drop_devices)\n",
    "\n",
    "def drop_dump_rows(data, vector):\n",
    "    total = 0\n",
    "    for device, count in data['device'].value_counts().sort_index().iteritems():\n",
    "        dumpveri = count % vector\n",
    "        dump = count - dumpveri\n",
    "        data = data.drop(data.index[dump+total:count+total])\n",
    "        total += dump\n",
    "    return data\n",
    "\n",
    "# drop the dump rows\n",
    "padsiztrain = drop_dump_rows(padsiztrain, vector)\n",
    "padlitrain = drop_dump_rows(padlitrain, vector)\n",
    "test = drop_dump_rows(test, vector)\n",
    "    \n",
    "# Calculate the total sum of the 'Size' column of the test dataset\n",
    "TestPadsizToplam = test['Size'].values.sum()\n",
    "\n",
    "# Pad function\n",
    "def pad_func(data, pad):\n",
    "    if pad == 100:\n",
    "        # apply padding strategy for pad value = 100\n",
    "        data['Size']=data['Size'].apply(lambda length : 100 if length <= 100  #level 100\n",
    "                                                    else (200 if 100<length <= 200 #level 100\n",
    "                                                    else (300 if 200<length <= 300 #level 100\n",
    "                                                    else ((length+random.randint(1, 1000-length)) if 300<length < 999 #All\n",
    "                                                    else ((length+random.randint(1, 1400-length)) if 999<=length <= 1399 #All\n",
    "                                                    else 1500 if 1400<=length  else 0    )))))  #All\n",
    "    elif pad in [300, 350, 500, 700, 900]:\n",
    "        # apply padding strategy for pad values = 300, 500, 700, 900\n",
    "        data['Size'] = data['Size'].apply(lambda length : pad if length <= pad  #level \n",
    "                                                    else ((length+random.randint(1, 1000-length)) if pad<length < 999 \n",
    "                                                    else ((length+random.randint(1, 1400-length)) if 999<=length <= 1399 \n",
    "                                                    else 1500 if 1400<=length  else 0))) #All\n",
    "    elif pad == 1000:\n",
    "        # apply padding strategy for pad value = 'random'\n",
    "         data['Size']=data['Size'].apply(lambda length : ((length+random.randint(1, 1500-length)) if length < 1500  #Random\n",
    "                                                    else 1500 ))\n",
    "    elif pad == 1500:\n",
    "        # apply padding strategy for pad value = 'MTU'\n",
    "        data['Size']=data['Size'].apply(lambda length : 1500   )  #MTU\n",
    "    else:\n",
    "        print(\"Please select the right padding name. (100,300,500,700,900, 1000(random) or 1500(MTU))\")\n",
    "\n",
    "# Apply padding\n",
    "if strategy == 1:\n",
    "    pad_func(padlitrain, pad)\n",
    "    pad_func(test, pad)\n",
    "    TestPadliToplam = test['Size'].values.sum()\n",
    "    Overhead = (TestPadliToplam - TestPadsizToplam) / TestPadsizToplam * 100\n",
    "\n",
    "# Function to prepare datasets\n",
    "def prepare_data(data, vector):\n",
    "    data['rowcount'] = np.arange(len(data))\n",
    "    list = [None] * vector\n",
    "    for i in range(vector):\n",
    "        list[i] = data.loc[data['rowcount'] % vector == i].reset_index().rename({'Size': 'Size'+str(i+1)}, axis='columns')\n",
    "    data = pd.concat([item for item in list], axis=1)\n",
    "    data = data.drop([ 'TIME','index','rowcount'], axis=1)\n",
    "    return data\n",
    "\n",
    "# Prepare datasets\n",
    "padlitrain = prepare_data(padlitrain, vector)\n",
    "padsiztrain = prepare_data(padsiztrain, vector)\n",
    "test = prepare_data(test, vector)\n",
    "\n",
    "# Create copies of the dataframes dropping the 'device' column\n",
    "padlitrain1 = padlitrain.drop(columns=['device','level_0'])\n",
    "padlitrainFPA = padlitrain.drop(columns=['device'])\n",
    "test1 = test.drop(columns=['device','level_0'])\n",
    "testFPA = test.drop(columns=['device'])\n",
    "padsiztrain1 = padsiztrain.drop(columns=['device'])\n",
    "\n",
    "# Get 'device' columns from test and padsiztrain dataframes\n",
    "testdevice = test.iloc[:, -1]\n",
    "padsiztraindevice = padsiztrain.iloc[:, -1]\n",
    "\n",
    "# Reset index and drop 'index' column from padlitrainFPA and testFPA dataframes\n",
    "padlitrainFPA = padlitrainFPA.reset_index().drop(['index', 'level_0'], axis=1)\n",
    "testFPA = testFPA.reset_index().drop(['index', 'level_0'], axis=1)\n",
    "padsiztrain1 = padsiztrain1.reset_index().drop(['index', 'level_0'], axis=1)\n",
    "\n",
    "# Get 'device' column from padlitrain dataframe\n",
    "traindevice = padlitrain.iloc[:, -1]\n",
    "        \n",
    "# Strategy 0 section where FPA is performed\n",
    "def FPAfunc(epsilon,k):\n",
    "    # DFT\n",
    "    for i in range (len(padlitrain)):\n",
    "        l2sen=padlitrain1.iloc[i].values.max()-padlitrain1.iloc[i].values.min()\n",
    "        padlitrainFPAold = np.fft.rfft(padlitrainFPA.iloc[i].transpose())\n",
    "        n=len(padlitrainFPAold)\n",
    "        padlitrainFPAnow=padlitrainFPAold[:k]\n",
    "        \n",
    "        #Laplace\n",
    "        sensitivity = math.sqrt(k)*l2sen #arttırınca karışıklık artıyor\n",
    "        noise_real = np.random.laplace(loc=0, scale=sensitivity/epsilon, size=np.shape(padlitrainFPAnow))\n",
    "        noise_imag = np.random.laplace(loc=0, scale=sensitivity/epsilon, size=np.shape(padlitrainFPAnow))\n",
    "        padlitrainFPAnow.real+=noise_real\n",
    "        padlitrainFPAnow.imag+=noise_imag\n",
    "\n",
    "        #pad\n",
    "        padlen=n-k\n",
    "        fftpad=np.pad(padlitrainFPAnow, pad_width=padlen, mode='constant', constant_values=0)[padlen:] \n",
    "\n",
    "        #IDFT\n",
    "        padlitrainFPAnow = np.fft.irfft(fftpad) \n",
    "        padlitrainFPAnow = np.clip(padlitrainFPAnow,clip_bottom,clip_top)\n",
    "        padlitrainFPA.iloc[i]=padlitrainFPAnow.transpose()\n",
    "    \n",
    "    # DFT\n",
    "    for i in range (len(test)):\n",
    "        l2sen=test1.iloc[i].values.max()-test1.iloc[i].values.min()\n",
    "        testFPAold = np.fft.rfft(testFPA.iloc[i].transpose())\n",
    "        n=len(testFPAold)\n",
    "        testFPAnow=testFPAold[:k]\n",
    "        \n",
    "        #Laplace\n",
    "        sensitivity = math.sqrt(k)*l2sen #arttırınca karışıklık artıyor\n",
    "        noise_real = np.random.laplace(loc=0, scale=sensitivity/epsilon, size=np.shape(testFPAnow))\n",
    "        noise_imag = np.random.laplace(loc=0, scale=sensitivity/epsilon, size=np.shape(testFPAnow))\n",
    "        testFPAnow.real+=noise_real\n",
    "        testFPAnow.imag+=noise_imag\n",
    "\n",
    "        #pad\n",
    "        padlen=n-k\n",
    "        fftpad=np.pad(testFPAnow, pad_width=padlen, mode='constant', constant_values=0)[padlen:] \n",
    "\n",
    "        #IDFT\n",
    "        testFPAnow = np.fft.irfft(fftpad)\n",
    "        testFPAnow = np.clip(testFPAnow,clip_bottom,clip_top)\n",
    "        testFPA.iloc[i]=testFPAnow.transpose()\n",
    "    \n",
    "    #################FPA Test##########################\n",
    "\n",
    "    \n",
    "    \n",
    "##################################################\n",
    "if strategy == 0:   \n",
    "    FPAfunc(epsilon,k)\n",
    "    TestPadliToplamFPA=testFPA.values.sum()\n",
    "    Overhead=(TestPadliToplamFPA-TestPadsizToplam)/TestPadsizToplam*100\n",
    "else:\n",
    "    pass\n",
    "pd.set_option('display.float_format', lambda x: '%.f' % x)\n",
    "testFPA = pd.concat([testFPA, testdevice], axis=1)\n",
    "padlitrainFPA = pd.concat([padlitrainFPA, traindevice], axis=1)\n",
    "padsiztrain1= pd.concat([padsiztrain1, padsiztraindevice], axis=1) \n",
    "\n",
    "\n",
    "#XGBoost#\n",
    "def run_xgb(train_X, train_y, test_X, test_y, observer_type):\n",
    "    # Initialize the XGBoost model\n",
    "    model = xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=50, random_state=101) \n",
    "\n",
    "    # Setup k-fold cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=101)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    with parallel_backend('threading', n_jobs=-1):\n",
    "        scores = cross_val_score(model, train_X, train_y.values.ravel(), cv=kfold, scoring='accuracy')\n",
    "\n",
    "    # Display accuracy for each fold\n",
    "    for i, score in enumerate(scores, 1):\n",
    "        print(f'Accuracy for fold {i}: {score*100:.2f}%')\n",
    "        \n",
    "    # Train the model with the full training set\n",
    "    model.fit(train_X, train_y.values.ravel()) \n",
    "\n",
    "    # Make predictions on the train and test sets\n",
    "    predicted_train_y = model.predict(train_X)\n",
    "    predicted_test_y = model.predict(test_X)\n",
    "    \n",
    "    # Calculate and print mean training accuracy and variance\n",
    "    mean_train_accuracy = np.mean(scores) * 100\n",
    "    print(f'\\nAverage training accuracy after 3-fold cross-validation: {mean_train_accuracy:.2f}%')\n",
    "    print(f'Variance of test accuracy after 3-fold cross-validation: {np.var(scores) * 100:.2f}%')\n",
    "\n",
    "    # Print observer type\n",
    "    if strategy == 1:  # Padding\n",
    "        print(f\"\\n------XGBoost Classifier (Vector Size: {vector}, Pad: {pad}, Observer: {observer_type}) ------\")\n",
    "    elif strategy == 0:  # FPA\n",
    "        print(f\"\\n------XGBoost Classifier (Vector Size: {vector}, FPA parameters: k={k}, epsilon={epsilon:.2f}, {observer_type} Observer) ------\")\n",
    "    else:\n",
    "        pass\n",
    "    # Print the accuracy scores for the train and test sets\n",
    "    train_accuracy = accuracy_score(train_y, predicted_train_y) * 100\n",
    "    test_accuracy = accuracy_score(test_y, predicted_test_y) * 100\n",
    "    print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"XGBoost Classification Report:\\n{classification_report(test_y, predicted_test_y, zero_division=0)}\")\n",
    "\n",
    "    # Print byte overhead\n",
    "    print(f\"Byte Overhead: {Overhead}%\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(test_y, predicted_test_y)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.gcf().set_size_inches(11, 11)\n",
    "    plt.show()\n",
    "    \n",
    "#LSTM#\n",
    "def run_lstm(train_X, train_y, test_X, test_y, observer_type):\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    encoder = LabelEncoder()\n",
    "    train_y = encoder.fit_transform(train_y)\n",
    "    test_y = encoder.transform(test_y)\n",
    "    train_y = to_categorical(train_y)\n",
    "    test_y = to_categorical(test_y)\n",
    "\n",
    "    # Reshape the input data\n",
    "    train_X = train_X.reshape(-1, 1, vector)\n",
    "    test_X = test_X.reshape(-1, 1, vector)\n",
    "\n",
    "    # Create the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, activation='relu', input_shape=(1, vector)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(17, activation='softmax'))\n",
    "    earlyStop = EarlyStopping(monitor=\"val_loss\", verbose=2, mode='min', patience=10)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # Cross-validation during training\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    cv_scores = []  # List to store cross-validation scores\n",
    "\n",
    "    for train_index, val_index in kf.split(train_X):\n",
    "        X_train, X_val = train_X[train_index], train_X[val_index]\n",
    "        y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "        print(f\"\\nTraining for Fold {fold}\")\n",
    "        history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), verbose=2, callbacks=[earlyStop])\n",
    "\n",
    "        # Evaluate on validation data for the current fold\n",
    "        _, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        cv_scores.append(val_acc)  # Store validation accuracy for the fold\n",
    "        fold += 1\n",
    "\n",
    "    # Calculate and print mean cross-validation accuracy and variance\n",
    "    mean_cv_accuracy = np.mean(cv_scores) * 100\n",
    "    var_cv_accuracy = np.var(cv_scores) * 100\n",
    "    print(f\"\\nAverage cross-validation accuracy after 3-fold cross-validation: {mean_cv_accuracy:.2f}%\")\n",
    "    print(f\"Variance of cross-validation accuracy after 3-fold cross-validation: {var_cv_accuracy:.2f}%\")\n",
    "\n",
    "    # Evaluate on test data\n",
    "    _, test_acc = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    if strategy == 1:  # Padding\n",
    "        print(f\"\\n------LSTM (Vector Size: {vector}, Pad: {pad}, Observer: {observer_type}) ------\")\n",
    "    elif strategy == 0:  # FPA\n",
    "        print(f\"\\n------LSTM (Vector Size: {vector}, FPA parameters: k={k}, epsilon={epsilon:.2f}, Observer: {observer_type}) ------\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Print training accuracy, test accuracy, and classification report\n",
    "    train_pred = model.predict(train_X)\n",
    "    test_pred = model.predict(test_X)\n",
    "\n",
    "    train_acc = accuracy_score(np.argmax(train_y, axis=1), np.argmax(train_pred, axis=1)) * 100\n",
    "    test_acc = accuracy_score(np.argmax(test_y, axis=1), np.argmax(test_pred, axis=1)) * 100\n",
    "\n",
    "    print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(\"LSTM Classification Report:\")\n",
    "    print(classification_report(np.argmax(test_y, axis=1), np.argmax(test_pred, axis=1), zero_division=0))\n",
    "    print(\"Byte Overhead: \", Overhead, \"%\")\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#XGBoost For Internal observer\n",
    "train_X = padlitrainFPA.drop(columns=['device'])\n",
    "train_y = padlitrainFPA[['device']]\n",
    "test_X = testFPA.drop(columns=['device'])\n",
    "test_y = testFPA[['device']]\n",
    "run_xgb(train_X, train_y, test_X, test_y, 'Internal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#XGBoost For External observer\n",
    "train_X = padsiztrain1.drop(columns=['device'])\n",
    "train_y = padsiztrain1[['device']]\n",
    "test_X = testFPA.drop(columns=['device'])\n",
    "test_y = testFPA[['device']]\n",
    "run_xgb(train_X, train_y, test_X, test_y, 'External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6227c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#LSTM For Internal observer\n",
    "train_X = padlitrainFPA.drop(columns=['device'])\n",
    "train_y = padlitrainFPA[['device']]\n",
    "test_X = testFPA.drop(columns=['device'])\n",
    "test_y = testFPA[['device']]\n",
    "\n",
    "run_lstm(train_X, train_y, test_X, test_y, 'Internal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56417fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#LSTM For External observer\n",
    "train_X = padsiztrain1.drop(columns=['device'])\n",
    "train_y = padsiztrain1[['device']]\n",
    "test_X = testFPA.drop(columns=['device'])\n",
    "test_y = testFPA[['device']]\n",
    "\n",
    "run_lstm(train_X, train_y, test_X, test_y, 'External')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
